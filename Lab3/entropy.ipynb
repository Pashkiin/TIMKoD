{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rtoit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample0.txt, Entropy of words: 7.7487413861401295\n",
      "sample0.txt, Entropy of characters: 4.273001240566633\n",
      "sample1.txt, Entropy of words: 11.500700199498642\n",
      "sample1.txt, Entropy of characters: 4.1270061355497205\n",
      "sample2.txt, Entropy of words: 8.023869815826425\n",
      "sample2.txt, Entropy of characters: 3.9933118002325836\n",
      "sample3.txt, Entropy of words: 9.061122986146984\n",
      "sample3.txt, Entropy of characters: 3.9302978341579875\n",
      "sample4.txt, Entropy of words: 17.129669111070662\n",
      "sample4.txt, Entropy of characters: 4.253809567379015\n",
      "sample5.txt, Entropy of words: 16.509437287775633\n",
      "sample5.txt, Entropy of characters: 4.441688018481797\n"
     ]
    }
   ],
   "source": [
    "#anwsers for task 3 saved at the end inside file sample_results.txt\n",
    "#entropy of words and characters for each file and average,min,max values for words and characters\n",
    "#same as result of anylysing the norm files saved in norm_results.txt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.probability import FreqDist\n",
    "import math\n",
    "\n",
    "norm_files_list = ['norm_wiki_en.txt', 'norm_wiki_eo.txt', 'norm_wiki_et.txt', 'norm_wiki_ht.txt', 'norm_wiki_la.txt', 'norm_wiki_nv.txt', 'norm_wiki_so.txt']\n",
    "sample_files_list = ['sample0.txt', 'sample1.txt', 'sample2.txt', 'sample3.txt', 'sample4.txt', 'sample5.txt']\n",
    "work_file = 'sample_results.txt'\n",
    "\n",
    "#initialize lists to store entropy values\n",
    "entropy_words_list = []\n",
    "entropy_chars_list = []\n",
    "\n",
    "#clear the result file\n",
    "with open(work_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('')\n",
    "\n",
    "#repeat operation for all files in the list\n",
    "for file in sample_files_list:\n",
    "    # Open and read the file\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Tokenize the text into words and characters\n",
    "    words = nltk.word_tokenize(text)\n",
    "    chars = list(text)\n",
    "\n",
    "    # Calculate the frequency distribution\n",
    "    fdist_words = FreqDist(words)\n",
    "    fdist_chars = FreqDist(chars)\n",
    "\n",
    "    # Calculate the probabilities\n",
    "    prob_words = {word: freq / len(words) for word, freq in fdist_words.items()}\n",
    "    prob_chars = {char: freq / len(chars) for char, freq in fdist_chars.items()}\n",
    "\n",
    "    # Calculate the entropy\n",
    "    entropy_words = -sum(prob * math.log2(prob) for prob in prob_words.values())\n",
    "    entropy_chars = -sum(prob * math.log2(prob) for prob in prob_chars.values())\n",
    "\n",
    "    print(f'{file}, Entropy of words: {entropy_words}')\n",
    "    print(f'{file}, Entropy of characters: {entropy_chars}')\n",
    "\n",
    "    #save the results to a file\n",
    "    with open(work_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f'{file}, Entropy of words: {entropy_words}\\n')\n",
    "        f.write(f'{file}, Entropy of characters: {entropy_chars}\\n')\n",
    "\n",
    "    #add values to a list for later use\n",
    "    entropy_words_list.append(entropy_words)\n",
    "    entropy_chars_list.append(entropy_chars)\n",
    "\n",
    "#calculate the average,min,max entropy for words and characters across files\n",
    "average_entropy_words = sum(entropy_words_list) / len(entropy_words_list)\n",
    "average_entropy_chars = sum(entropy_chars_list) / len(entropy_chars_list)\n",
    "min_entropy_words = min(entropy_words_list)\n",
    "min_entropy_chars = min(entropy_chars_list)\n",
    "max_entropy_words = max(entropy_words_list)\n",
    "max_entropy_chars = max(entropy_chars_list)\n",
    "\n",
    "#add values to a file\n",
    "with open(work_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f'\\nAverage entropy of words: {average_entropy_words}\\n')\n",
    "    f.write(f'Average entropy of characters: {average_entropy_chars}\\n')\n",
    "    f.write(f'Min entropy of words: {min_entropy_words}\\n')\n",
    "    f.write(f'Min entropy of characters: {min_entropy_chars}\\n')\n",
    "    f.write(f'Max entropy of words: {max_entropy_words}\\n')\n",
    "    f.write(f'Max entropy of characters: {max_entropy_chars}\\n\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample0.txt, Conditional entropy of bigrams: 2.9158940043476744\n",
      "sample0.txt, Conditional entropy of trigrams: 2.0003592449340295\n",
      "sample0.txt, Conditional entropy of quadgrams: 1.5392818308831178\n",
      "sample0.txt, Conditional entropy of bigrams words: 7.486391705103802\n",
      "sample0.txt, Conditional entropy of trigrams words: 4.406703436668593\n",
      "sample0.txt, Conditional entropy of quadgrams words: 0.5950075484368974\n",
      "sample1.txt, Conditional entropy of bigrams: 3.2391499921232665\n",
      "sample1.txt, Conditional entropy of trigrams: 2.861279684784113\n",
      "sample1.txt, Conditional entropy of quadgrams: 2.326684740548095\n",
      "sample1.txt, Conditional entropy of bigrams words: 5.372246627142164\n",
      "sample1.txt, Conditional entropy of trigrams words: 1.5747363489998651\n",
      "sample1.txt, Conditional entropy of quadgrams words: 0.5075093857038041\n",
      "sample2.txt, Conditional entropy of bigrams: 3.050439282780695\n",
      "sample2.txt, Conditional entropy of trigrams: 2.467660235739306\n",
      "sample2.txt, Conditional entropy of quadgrams: 1.9397723261083073\n",
      "sample2.txt, Conditional entropy of bigrams words: 7.348623164061675\n",
      "sample2.txt, Conditional entropy of trigrams words: 3.7819360481715925\n",
      "sample2.txt, Conditional entropy of quadgrams words: 0.8595046784849137\n",
      "sample3.txt, Conditional entropy of bigrams: 3.18446706709535\n",
      "sample3.txt, Conditional entropy of trigrams: 2.627895709506982\n",
      "sample3.txt, Conditional entropy of quadgrams: 2.023991488550604\n",
      "sample3.txt, Conditional entropy of bigrams words: 5.950222753530311\n",
      "sample3.txt, Conditional entropy of trigrams words: 2.6308038817857557\n",
      "sample3.txt, Conditional entropy of quadgrams words: 1.2640890740095818\n",
      "sample4.txt, Conditional entropy of bigrams: 4.229101430962342\n",
      "sample4.txt, Conditional entropy of trigrams: 4.226828937890713\n",
      "sample4.txt, Conditional entropy of quadgrams: 4.178535148281953\n",
      "sample4.txt, Conditional entropy of bigrams words: 3.4442531121956796\n",
      "sample4.txt, Conditional entropy of trigrams words: 0.23407523294515523\n",
      "sample4.txt, Conditional entropy of quadgrams words: 0.0032266435185575003\n",
      "sample5.txt, Conditional entropy of bigrams: 3.5230981260850065\n",
      "sample5.txt, Conditional entropy of trigrams: 3.250620854647967\n",
      "sample5.txt, Conditional entropy of quadgrams: 2.834271485616726\n",
      "sample5.txt, Conditional entropy of bigrams words: 0.0001366790394469873\n",
      "sample5.txt, Conditional entropy of trigrams words: -3.085078184373857e-08\n",
      "sample5.txt, Conditional entropy of quadgrams words: -3.085080250872073e-08\n"
     ]
    }
   ],
   "source": [
    "#initialize lists for bigrams and trigrams and quadgrams\n",
    "entropy_bigrams_list = []\n",
    "entropy_trigrams_list = []\n",
    "entropy_quadgrams_list = []\n",
    "entropy_bigrams_words_list = []\n",
    "entropy_trigrams_words_list = []\n",
    "entropy_quadgrams_words_list = []\n",
    "\n",
    "#repeat operation for all files in the list\n",
    "for file in sample_files_list:\n",
    "     # Open and read the file\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "           \n",
    "    # Function to generate n-grams\n",
    "    def generate_ngrams(text, n):\n",
    "        return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "    \n",
    "    # Function to generade words n-grams\n",
    "    def generate_ngrams_words(text, n):\n",
    "        words = []\n",
    "        words = nltk.word_tokenize(text)\n",
    "        return [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "    \n",
    "    # Tokenize the text into words and characters\n",
    "    words = nltk.word_tokenize(text)\n",
    "    chars = list(text)\n",
    "\n",
    "    # Calculate the frequency distribution\n",
    "    fdist_words = FreqDist(words)\n",
    "    fdist_chars = FreqDist(chars)\n",
    "\n",
    "    # Calculate the probabilities\n",
    "    prob_words = {word: freq / len(words) for word, freq in fdist_words.items()}\n",
    "    prob_chars = {char: freq / len(chars) for char, freq in fdist_chars.items()}\n",
    "\n",
    "    # Generate bigrams and trigrams and quadgrams for characters and words\n",
    "    bigrams = generate_ngrams(text, 2)\n",
    "    trigrams = generate_ngrams(text, 3)\n",
    "    quadgrams = generate_ngrams(text, 4)\n",
    "    bigrams_words = generate_ngrams_words(text, 2)\n",
    "    trigrams_words = generate_ngrams_words(text, 3)\n",
    "    quadgrams_words = generate_ngrams_words(text, 4)\n",
    "\n",
    "    # Calculate the frequency distribution\n",
    "    fdist_bigrams = FreqDist(bigrams)\n",
    "    fdist_trigrams = FreqDist(trigrams)\n",
    "    fdist_quadgrams = FreqDist(quadgrams)\n",
    "    fdist_bigrams_words = FreqDist(bigrams_words)\n",
    "    fdist_trigrams_words = FreqDist(trigrams_words)\n",
    "    fdist_quadgrams_words = FreqDist(quadgrams_words)\n",
    "\n",
    "    # Calculate the conditional probabilities\n",
    "    prob_bigrams = {bigram: freq / len(bigrams) for bigram, freq in fdist_bigrams.items()}\n",
    "    prob_trigrams = {trigram: freq / len(trigrams) for trigram, freq in fdist_trigrams.items()}\n",
    "    prob_quadgrams = {quadgram: freq / len(quadgrams) for quadgram, freq in fdist_quadgrams.items()}\n",
    "    prob_bigrams_words = {bigram: freq / len(bigrams_words) for bigram, freq in fdist_bigrams_words.items()}\n",
    "    prob_trigrams_words = {trigram: freq / len(trigrams_words) for trigram, freq in fdist_trigrams_words.items()}\n",
    "    prob_quadgrams_words = {quadgram: freq / len(quadgrams_words) for quadgram, freq in fdist_quadgrams_words.items()}\n",
    "\n",
    "    # Calculate the conditional entropy\n",
    "    entropy_bigrams = -sum(prob * math.log2(prob / prob_chars[bigram[0]]) for bigram, prob in prob_bigrams.items())\n",
    "    entropy_trigrams = -sum(prob * math.log2(prob / prob_bigrams[trigram[:2]]) for trigram, prob in prob_trigrams.items())\n",
    "    entropy_quadgrams = -sum(prob * math.log2(prob / prob_trigrams[quadgram[:3]]) for quadgram, prob in prob_quadgrams.items())\n",
    "    entropy_bigrams_words = -sum(prob * math.log2(prob / prob_words[bigram.split()[0]]) for bigram, prob in prob_bigrams_words.items())\n",
    "    entropy_trigrams_words = -sum(prob * math.log2(prob / prob_bigrams_words[' '.join(trigram.split()[:2])]) for trigram, prob in prob_trigrams_words.items())\n",
    "    entropy_quadgrams_words = -sum(prob * math.log2(prob / prob_trigrams_words[' '.join(quadgram.split()[:3])]) for quadgram, prob in prob_quadgrams_words.items())\n",
    "\n",
    "    print(f'{file}, Conditional entropy of bigrams: {entropy_bigrams}')\n",
    "    print(f'{file}, Conditional entropy of trigrams: {entropy_trigrams}')\n",
    "    print(f'{file}, Conditional entropy of quadgrams: {entropy_quadgrams}')\n",
    "    print(f'{file}, Conditional entropy of bigrams words: {entropy_bigrams_words}')\n",
    "    print(f'{file}, Conditional entropy of trigrams words: {entropy_trigrams_words}')\n",
    "    print(f'{file}, Conditional entropy of quadgrams words: {entropy_quadgrams_words}')\n",
    "\n",
    "    #save the results to a file\n",
    "    with open(work_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f'{file}, Conditional entropy of bigrams: {entropy_bigrams}\\n')\n",
    "        f.write(f'{file}, Conditional entropy of trigrams: {entropy_trigrams}\\n')\n",
    "        f.write(f'{file}, Conditional entropy of quadgrams: {entropy_quadgrams}\\n')\n",
    "        f.write(f'{file}, Conditional entropy of bigrams words: {entropy_bigrams_words}\\n')\n",
    "        f.write(f'{file}, Conditional entropy of trigrams words: {entropy_trigrams_words}\\n')\n",
    "        f.write(f'{file}, Conditional entropy of quadgrams words: {entropy_quadgrams_words}\\n\\n')\n",
    "\n",
    "    #add values to a list for later use\n",
    "    entropy_bigrams_list.append(entropy_bigrams)\n",
    "    entropy_trigrams_list.append(entropy_trigrams)\n",
    "    entropy_quadgrams_list.append(entropy_quadgrams)\n",
    "    entropy_bigrams_words_list.append(entropy_bigrams_words)\n",
    "    entropy_trigrams_words_list.append(entropy_trigrams_words)\n",
    "    entropy_quadgrams_words_list.append(entropy_quadgrams_words)\n",
    "\n",
    "#calculate the average,min,max entropy for bigrams and trigrams across files for characters\n",
    "average_entropy_bigrams = sum(entropy_bigrams_list) / len(entropy_bigrams_list)\n",
    "average_entropy_trigrams = sum(entropy_trigrams_list) / len(entropy_trigrams_list)\n",
    "average_entropy_quadgrams = sum(entropy_quadgrams_list) / len(entropy_quadgrams_list)\n",
    "min_entropy_bigrams = min(entropy_bigrams_list)\n",
    "min_entropy_trigrams = min(entropy_trigrams_list)\n",
    "min_entropy_quadgrams = min(entropy_quadgrams_list)\n",
    "max_entropy_bigrams = max(entropy_bigrams_list)\n",
    "max_entropy_trigrams = max(entropy_trigrams_list)\n",
    "max_entropy_quadgrams = max(entropy_quadgrams_list)\n",
    "\n",
    "#calculate the average,min,max entropy for bigrams and trigrams across files for words\n",
    "average_entropy_bigrams_words = sum(entropy_bigrams_words_list) / len(entropy_bigrams_words_list)\n",
    "average_entropy_trigrams_words = sum(entropy_trigrams_words_list) / len(entropy_trigrams_words_list)\n",
    "average_entropy_quadgrams_words = sum(entropy_quadgrams_words_list) / len(entropy_quadgrams_words_list)\n",
    "min_entropy_bigrams_words = min(entropy_bigrams_words_list)\n",
    "min_entropy_trigrams_words = min(entropy_trigrams_words_list)\n",
    "min_entropy_quadgrams_words = min(entropy_quadgrams_words_list)\n",
    "max_entropy_bigrams_words = max(entropy_bigrams_words_list)\n",
    "max_entropy_trigrams_words = max(entropy_trigrams_words_list)\n",
    "max_entropy_quadgrams_words = max(entropy_quadgrams_words_list)\n",
    "\n",
    "#add values to a file\n",
    "with open(work_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f'\\nAverage conditional entropy of bigrams: {average_entropy_bigrams}\\n')\n",
    "    f.write(f'Average conditional entropy of trigrams: {average_entropy_trigrams}\\n')\n",
    "    f.write(f'Average conditional entropy of quadgrams: {average_entropy_quadgrams}\\n')\n",
    "    f.write(f'Min conditional entropy of bigrams: {min_entropy_bigrams}\\n')\n",
    "    f.write(f'Min conditional entropy of trigrams: {min_entropy_trigrams}\\n')\n",
    "    f.write(f'Min conditional entropy of quadgrams: {min_entropy_quadgrams}\\n')\n",
    "    f.write(f'Max conditional entropy of bigrams: {max_entropy_bigrams}\\n')\n",
    "    f.write(f'Max conditional entropy of trigrams: {max_entropy_trigrams}\\n')\n",
    "    f.write(f'Max conditional entropy of quadgrams: {max_entropy_quadgrams}\\n\\n')\n",
    "    f.write(f'Average conditional entropy of bigrams words: {average_entropy_bigrams_words}\\n')\n",
    "    f.write(f'Average conditional entropy of trigrams words: {average_entropy_trigrams_words}\\n')\n",
    "    f.write(f'Average conditional entropy of quadgrams words: {average_entropy_quadgrams_words}\\n')\n",
    "    f.write(f'Min conditional entropy of bigrams words: {min_entropy_bigrams_words}\\n')\n",
    "    f.write(f'Min conditional entropy of trigrams words: {min_entropy_trigrams_words}\\n')\n",
    "    f.write(f'Min conditional entropy of quadgrams words: {min_entropy_quadgrams_words}\\n')\n",
    "    f.write(f'Max conditional entropy of bigrams words: {max_entropy_bigrams_words}\\n')\n",
    "    f.write(f'Max conditional entropy of trigrams words: {max_entropy_trigrams_words}\\n')\n",
    "    f.write(f'Max conditional entropy of quadgrams words: {max_entropy_quadgrams_words}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
